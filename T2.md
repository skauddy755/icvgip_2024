> Recent trends in AI for remote sensing image analysis
> by Proff. Biplav Banerjee, IIT Bombay

# About the field and prior work

Remote sensing uses hyperspectral data, as the input. It is different from RGB since it has channels corresponding to multiple frequencies of the spectrum (possible beyond VIBGYOR as well). This data (satellite imaging) can be used to detect various objects like road, buildings, or can also be used for change detection to estimate flood, landslides and air raids.

Traditionally following methods have been used in this field:

1. MLP (pixel as independent entities)
2. CNN-2D (considering spatial and spectral aspects as well)
3. Hybrd Architecture for HSI Classification that includes both Conv-2D and Conv-3D
4. ViT (uses separate spectral tokens and spatial tokens)
5. HSI and LIDAR Fusion

# Our Contribution in Remote Sensing

## 1. FusAtNet: Dual Attention based SpectroSpatial Multimodal Fusion Network for Hyperspectral and LiDAR Classification

[link to paper](https://openaccess.thecvf.com/content_CVPRW_2020/papers/w6/Mohla_FusAtNet_Dual_Attention_Based_SpectroSpatial_Multimodal_Fusion_Network_for_Hyperspectral_CVPRW_2020_paper.pdf)

- We can leverage multimodal data in Remote Sensing (RS) where many data types like multispectral imagery (MSI), hyperspectral imagery (HSI), LiDAR etc. are available.
- Effective fusion of these multisource datasets is becoming important
- However, fusion in the context of RS is non-trivial due to:
    1. Redundancy involved in the data and
    2. Large domain differences among multiple modalities
    3. Feature extraction modules for different modalities hardly interact among themselves, which further limits their semantic relatedness.

- We propose a feature fusion and extraction framework, namely FusAtNet, for collective land-cover classification of HSIs and LiDAR data:
    1. "HSI modality" is used to generate an attention map using “self-attention” that highlights its own "spectral features".
    2. “cross-attention” approach is simultaneously used to harness the "LiDAR derived" attention map that "accentuates the spatial features of HSI"

![alt text](archive/image_1.png)

## 2. Coupled Feedback Network based HSI-LIDAR Fusion with Self Supervision

[link to a similar paper by same author(s)](https://www.sciencedirect.com/science/article/abs/pii/S0893608023002058)
- Uses a DenseNet based architecture which has connections between high and low resolution branches.
- Self supervision is used
- Cross modal representations between spatial and hyperspectral domain.


## 3. GAFNet: Improving the Performance of Remote Sensing Image Fusion using Novel Global Self and Cross Attention Learning

[link to paper](https://openaccess.thecvf.com/content/WACV2023/papers/Jha_GAF-Net_Improving_the_Performance_of_Remote_Sensing_Image_Fusion_Using_WACV_2023_paper.pdf)

PROBLEMS WITH EARLIER APPROACH:
- while the selfattention models fail to incorporate the global context due to the limited size of the receptive fields cross-attention learning may generate ambiguous features as the feature extractors for all the modalities are jointly trained.

PROPOSED SOLUTION:
- To address these issues, we propose a novel fusion architecture called Global Attention based Fusion Network (GAF-Net), equipped with novel self and cross-attention learning techniques.
- This is ViT based.
![alt text](archive/image_2.png)

CONTRIBUTION:
- We design a simple and generic bi-modal fusion network for RS data called GAF-Net to learn discriminative and compact features through novel attention learning-based feature refinement in a principled manner.
- To our knowledge, we propose the first non-local spectralspatial self-attention learning module using key-value processing Besides, we introduce the novel paradigm of crossattention learning from auxiliary tasks. Finally, we propose
explicitly reducing redundancy between the modality features through a novel regularizer.
- We compare our attention modules with existing counterparts on a variety of datasets (visual, audio, and depth modalities), showing that the proposed global self-attention convincingly beats the models based on local operations

![alt text](archive/image_3.png)

---

> An interesting way to see Residual Connections:
>   - KD (teacher-student model) -> distilling info from large network to a small network
>   - Self KD (model regularizations) -> distilling info from many neurons to maybe a few neurons (eg: during dropout)
>   - Self-KD within a Network -> distilling info from Layer1 + Layer2 + Layer3 into just Layer-1 if there is a residual connection between L1-L2 to L2-L3

> There is some work in remote sensing where we use Instance Normalization to capture the intrinsic parameters of various sensors. This is particularly helpful if we want to combine datasets from differnt sensors to create a training set.

> [Good at captioning, bad at counting: Benchmarking GPT-4V on Earth observation data](https://arxiv.org/pdf/2401.17600) Large Vision-Language Models (VLMs) have demonstrated impressive performance on complex tasks involving visual input with natural language instructions. However, it remains unclear to what extent capabilities on natural images transfer to Earth observation (EO) data, which are predominantly satellite and aerial images less common in VLM training data. In this work, we propose a comprehensive benchmark to gauge the progress of VLMs toward being useful tools for EO data by assessing their abilities on scene understanding, localization and counting, and change detection tasks.

# Foundation Models (Vision Only Models)


## 1. Masked Auto Encoder (MAE) [link to paper](https://arxiv.org/pdf/2111.06377)

- Divide the image into patches.
- Masking about 75% patches
- Positional Encoding should also be present for the patches.
- MAE encoder (ViT based) generates embeddings for the patches.
- MAE decoder tries reconstructs the original image. MSE Loss is applied as objective function.

    ![alt text](archive/image_4.png)

## 2. SAT MAE [link to paper](https://arxiv.org/pdf/2207.08051)

Developing unsupervised techniques for satellite imagery presents significant opportunities as unlabelled data is plentiful and the inherent temporal and multi-spectral structure provides avenues to further improve existing pre-training strategies. In this paper, we present SatMAE, a pre-training framework for temporal or multi-spectral satellite imagery based on Masked Autoencoder (MAE). 

1. To leverage temporal/spectral information, we include a temporal embedding
2. Independently masking image patches across temporal/spectral dimension. (There are actually 2 ways of masking: (i) constant/unified masking in which all samples in a batch are masked at same regions. (ii) Choice of region to mask is independent for each sample image in a batch)
3.  In addition, we demonstrate that encoding multi-spectral data as groups of bands with distinct spectral positional encodings is beneficial. (NOTE: Satellite imagery can be best represented as MS data with many bands (say C = 13) instead of the 3 channel RGB)

    ![alt text](archive/image_5.png)

## 3. SAT MAE ++ [link to paper](https://arxiv.org/pdf/2403.05419)

> Limitations of SATMAE: Although SATMAE operates well on RGB and multi-spectral data, however
> 1. it does not exploit the multi-scale information present in the remote sensing data.
> 2. In addition, it struggles to generalize across domains having images at multiple scale levels.
> 3. Furthermore, ScaleMAE shows that the scale information can be encoded through a GSD based positional embeddings. However, the GSD based positional embeddings introduced by ScaleMAE can only be used with the RGB data that has same GSD resolution for each channel. Therefore, it is desired to rethink the design of the SatMAE framework to learn the multi-scale information that is not constrained to single data modality.

- Remote Sensing data is acquired from various sensor technologies and exhibit diverse range of scale variations as well as modalities.
- Existing satellite image pre-training methods either ignore the scale information present in the remote sensing imagery or restrict themselves to use only a single type of data modality.
- In this paper, we re-visit transformers pre-training and leverage multi-scale information (since MS data has C=13 channels and the resolution is not same) that is effectively utilized with multiple modalities.

    ![alt text](archive/image_6.png)

## 4. Spectral GPT [link to paper](https://arxiv.org/pdf/2311.07113)

- Trained on 3D data by masking the 3D Cube
- Uses "incremental learning": training on one dataset and then on other.
- To avoid "catastrophic forgetting" due to increamental learning, we use weight regularization.

> Side Reference:
> 1. [Paper on CROSSGRAD](https://arxiv.org/abs/1804.10745)
> 2. [Paper on Gradient Surgery](https://arxiv.org/pdf/2001.06782)

## MM Earth [link to paper](https://arxiv.org/pdf/2405.02771v1)

- PROBLEM: The volume of unlabelled Earth observation (EO) data is huge, but many important applications lack labelled training data.
- MERIT THAT CAN BE EXPLOITED:  EO data offers the unique opportunity to pair data from different modalities and sensors automatically based on geographic location and time, at virtually no human labor cost.
- We seize this opportunity to:
    1. create a diverse multi-modal pretraining dataset at global scale.
    2. propose a Multi-Pretext Masked Autoencoder (MP-MAE) approach to learn general-purpose representations for optical satellite images.
- Our approach builds on the ConvNeXt V2 architecture, a fully convolutional masked autoencoder (MAE)

Here multi pretext means that, we reconstruct not only the image, but also other signals say depth-maps, segmentation-mask, etc.
    ![alt text](archive/image_7.png)

# Foundation Models (VLM based)

There can be many training strategies like:
1. Unsupervised pretraining & finetuning before prediction.
2. Supervised pretaining & finetuning before prediction.
3. VLM pretraining & zero-shot prediction

In VLMs, Nature of pretraining objectives can be: (1) Contrastive (2) Generative (3) Alignment

## CLIP [link to paper](https://arxiv.org/pdf/2103.00020)

- They tried 1 leg, 2 leg, fused achitecture for Vision-Language branches. 2-leg was better.
- Used contrastive loss (NOTE: It can be of two types Self sup or Supervised) In this paper they used SupCon.

> Side Reference: [SupCon](https://arxiv.org/pdf/2004.11362)

![alt text](archive/image_8.png)

- In contrastive loss, 1 +ve case is Okay, but many -ve cases are needed, for better generalizations.
- Multiple +ve and -ve anchors are used in CLIP.
- 2 ways of anchors have been used.
    1. img is anchored, +/- comes from text.
    2. text is anchored, +/- comes from image.

## Remote CLIP [link to paper](https://arxiv.org/pdf/2306.11029)


